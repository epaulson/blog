<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>HDFS Benchmarks</title>
  <meta name="author" content="Erik Paulson">

    
  
  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="http://epaulson.github.io/favicon.png" rel="icon">
  <link href="http://epaulson.github.io/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="http://epaulson.github.io/theme/js/modernizr-2.0.js"></script>
  <script src="http://epaulson.github.io/theme/js/ender.js"></script>
  <script src="http://epaulson.github.io/theme/js/octopress.js" type="text/javascript"></script>

  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head><body>
  <header role="banner"><hgroup>
  <h1><a href="http://epaulson.github.io/">Hadoop Internals</a></h1>
      <h2>a.k.a How the heck does that get called?</h2>
  </hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="http://epaulson.github.io/" rel="subscribe-rss">RSS</a></li>
  </ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:http://epaulson.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
  <li><a href="http://epaulson.github.io/">Blog</a></li>
            <!-- TODO: add categories here? -->
  </ul></nav>
  <div id="main">
    <div id="content">
      <div>
  <article class="hentry" role="article">
    <header>
          <h1 class="entry-title">HDFS Benchmarks</h1>
          <p class="meta"><time datetime="2013-08-28T10:00:00" pubdate>Wed 28 August 2013</time></p>
</header>

  <div class="entry-content"><hr />


<h3>Asking the Right Questions</h3>
<p>I'm interested in benchmarking the HDFS NameNode, and to be able to understand the impact of different mixes of operations and workloads. As part of this, I've been reviewing Hadoop Benchmarks. I've been trying to limit my focus to benchmarks that make the HDFS their primary focus, as opposed to benchmarks that try to be representative of a MapReduce workload. </p>
<p>What follows in this post are a few thoughts on the challenges of trying to focus on the NameNode in an HDFS benchmark, a quick survey of about 10 different benchmarks, and some ideas as to what could be included in a new benchmark for a NameNode or its equivalent in a distributed file system.
</p>
<h3>Contents</h3>
<ul>
<li><a href="#onbenchmarking">On Benchmarking</a></li>
<li>Specific HDFS Benchmarks<ul>
<li><a href="#dfsio">DFSIO</a></li>
<li><a href="#dfsio-e">DFSIO-e</a></li>
<li><a href="#nnbench">NNBench and NNBenchWithoutMR</a></li>
<li><a href="#slive">S-Live</a></li>
<li><a href="#loadgen">LoadGenerator</a></li>
<li><a href="#nnthroughput">NNThroughputBenchmark</a></li>
<li><a href="#testeditlog">TestEditLog</a></li>
<li><a href="#qfs">MStress, from Quantcast</a></li>
<li><a href="#osu">Ohio State Microbenchmarks</a></li>
<li><a href="#swim">Statistical Workload Injector for MapReduce</a></li>
</ul>
</li>
<li><a href="#newbenchmark">Towards a new benchmark for the NameNode and Namenode-like components</a><ul>
<li><a href="#qual1">Portable and easy to implementable on different systems</a></li>
<li><a href="#qual2">Do not require full stack</a></li>
<li><a href="#qual3">Separate data generation from benchmark driver</a></li>
<li><a href="#qual4">Operate on a mix of input data</a></li>
<li><a href="#qual5">Expose parallelism, but capture dependencies</a></li>
<li><a href="#qual6">Deterministic</a></li>
</ul>
</li>
</ul>
<h4><a name="onbenchmarking"></a>On Benchmarking</h4>
<p>HDFS was originally designed as a storage system that could coordinate a large number of commodity servers, and provide bulk streaming reads and writes to many hosts in parallel. This is still the primary use case, and fits well with the MapReduce programming model. When someone wants to "benchmark HDFS", they typically want the benchmark to run some bulk streaming IO scenario so they can use the benchmark numbers as a baseline to compare against another workload that may be more complicated to run. </p>
<p>Perhaps put another way, many HDFS benchmarks are meant to identify the bottleneck or saturation points in the various components and interconnects in the end to end system. If a simple identity map MapReduce job takes N seconds to process 500 Gigabytes of data, that can be used as a floor for understanding why a real MapReduce job takes N+M seconds. </p>
<p>In all of that data transfer, however, the NameNode's contribution can get lost. Instead of trying to put together a massive HDFS cluster to store legions of multi megabyte blocks, a NameNode benchmark needs to focus on being sure the NameNode is in fact being stressed. Rather than creating 100 one gigabyte files, a NameNode benchmark might better focus on creating many small or even empty files. </p>
<h3>Benchmarks</h3>
<h4><a name="dfsio"></a>DFSIO</h4>
<p>TestDFSIO, found in <code>hadoop-mapreduce-client-jobclient/src/test/j.o.a.h/fs</code>, is the canonical example of a benchmark that attempts to measure the HDFS's capacity for reading and writing bulk data. The test can measure the time taken to create a number of large files, and then turn around and use those files as inputs to a test to measure the read performance an HDFS instance can sustain. </p>
<p>The test runs with many nodes, each running a "thread" to handle reading and writing to separate files in parallel. The test is structured as a MapReduce program to handle the simultaneous launch of those threads as Map tasks. Each map task runs an instance of the test, creating or reading a file. Each map task notes the time it starts and completes the operation, and the size of the data it transfers, and divides the size by the time to compute a rate.  </p>
<p>The reduce task collect the results of the map and reports a summary of the benchmark, with the number of files and total bytes processed. It also reports two performance numbers: the "average I/O" and "throughput", both in megabytes a second. Now, you may wonder "aren't those basically the same thing", and the answer is yes. <a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/">Michael Noll's blog post</a> and the <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200901.mbox/%3C496EACE2.2090007@yahoo-inc.com%3E">thread he links to</a> describe the formula used to compute them, but it turns out that the "average I/O" is the arithmetic mean of the rates each map task calculated, and the "throughput" is the harmonic mean of the rates each map task calculated. </p>
<p>"Average I/O" and "throughput" are the means(averages) of performance individual execution slots in your cluster achieved. To understand the overall aggregate performance of your cluster, you must multiply those rates by the number of slots you have available for job execution. You also need to be sure that when you ran your benchmark, you had enough map tasks concurrently executing to fill those slots, so you can be sure that there is not a hidden bottleneck that running at full capacity would expose.     </p>
<h4><a name="dfsio-e"></a>HiBench/DFSIO-E</h4>
<p>DFSIO measurements are somewhat coarse, and report only a few summary statistics at the end and leave some uncertainty around any aggregate measurements of the cluster's performance. To address this, Intel included an "Enhanced DFSIO" as part of its <a href="https://github.com/intel-hadoop/HiBench">HiBench</a> suite of Hadoop benchmarks. As they describe DFSIO-E in their <a href="http://software.intel.com/sites/default/files/blog/329037/hibench-wbdb2012-updated.pdf">paper from the Workshop on Big Data Benchmarking in 2012</a>, DFSIO-E attempts to report an aggregate performance curve, rather than just a summary rate at the end. It does this by sampling performance frequently during each map task execution instead of reporting a single number per map task. DFSIO-E assumes a somewhat synced clock throughout the cluster, and uses that to order the different samples by timestamp and fit an overall rate curve to the samples. DFSIO-E is not part of the standard Hadoop test suite, but perhaps should be. </p>
<h4><a name="nnbench"></a>NNBench and NNBenchWithoutMR</h4>
<p>NNBench and its companion, NNBenchWithoutMR, are parts of the Hadoop Test Suite, found in <code>hadoop-mapreduce-client-jobclient/src/test/j.o.a.h/hdfs</code>. We'll start with NNBenchWithoutMR.</p>
<p>NNBenchWithoutMR is a single-threaded Java program that uses the Hadoop HDFS Client libraries to create, read, rename, and delete a collection of files, in that order. The size of the files created is configurable, though as the benchmark is meant to stress the NameNode file size is typically small. The actual data written (and in turn read) are all zeros, though the benchmark does not check to see that this is always preserved. </p>
<p>The benchmark starts by creating a unique working directory in HDFS. Then it moves on to the first sub-benchmark where for some provided number <code>N</code> files, it then creates a file, writes out the contents of the file, and closes the file before moving on to the next file in the test. The file is not replicated in HDFS, i.e. it has a replication factor of '1'.  All operations are blocking and no calls are issued asynchronously.  After writing <code>N</code> files, the benchmark can move on to the next sub-benchmarks over the <code>N</code> files where it opens and reads the contents of each file, renames each file, and deletes each file. Again, in each of the sub-benchmarks each file operation is completed before moving on to the next file, and each call is issued as a blocking call. At the end, the benchmark reports the total wall-clock time. </p>
<p>A slightly more sophisticated version of the benchmark, NNBench, is available that runs as a MapReduce job, much like TestDFSIO. In this version, the same core sub-benchmarks are used, with an enhancement that the benchmark records the time taken for each call that manipulates the file system. (This is not something that requires MapReduce, and could easily be done with the non-MapReduce version.) The user may also specify the replication factor for the files created. The MapReduce framework is used to launch multiple instances of the benchmark, each operating in its own unique working directory of HDFS. Each map task runs the benchmark once, and the number of map tasks created is specified by the user. A reduce tasks collects the benchmark results from each map task and computes overall statistics. </p>
<p>Load on the NameNode can be increased by running more benchmarks, either by executing more instances of the NNBenchWithoutMR, or by increasing the number of map tasks with NNBench. With the MapReduce version, there is no guarantee that the map tasks will all be run concurrently as intended, though this typically happens in practice. Both versions of the benchmark can pause until a specified wall-clock time and coordinate with a "synchronize watches" style barrier, to try to ensure all load is placed on the NameNode at the same time. </p>
<p>The NNBenchWithoutMR reports only wall-clock time for the entire benchmark. The MapReduce version is instrumented to track the time individual operations between the client and the NameNode take, and could be modified to report not only the average time but a more detailed distribution of those times. Unfortunately, because it instruments around the client library and not say the RPC layer, it may not have an accurate view of what's really occurring at the NameNode. It measures not only the time for the message to get from the benchmark to the NameNode, for the NameNode to process the message, and for the message response to return from the NameNode, but it also measures the time the client library marshals and unmarshals the messages and any other processing that may occur at the client. The Hadoop client libraries make heavy use of threads, and so non-determinism from queuing delay will also be included in measurement of client calls. The client library may also mask failures by automatically retrying failed operations, creating the appearance of a single call that took twice as long (or longer) rather than two or more calls between the client and NameNode, and accurately tracking the time each took. </p>
<p>The NNBench benchmarks deterministically construct their environment in a way that requires them to track little state but yet still be able to quickly decide on which files to operate. Files are created in the first step, which allows the benchmark to be sure the files are available, and follow a naming convention that allow it to easily compute a file to successfully read. Similarly, the inputs to the rename and delete operations are readily available with little more than a base filename and a counter in the benchmark. </p>
<h4><a name="slive"></a>S-Live</h4>
<p>The Stress Test for Live Data Verification, or S-Live, is a more sophisticated benchmark than the NNBench set of benchmarks. It follows the same construction of DFSIO and NNBench in that it operates a basic benchmark many times in parallel using MapReduce, with the core benchmark as the map task. The reduce tasks summarize the results of the benchmark runs conducted by the maps. It was developed by <a href="http://people.apache.org/~shv/Publications.html">Konstantin Shvachko</a>, lately of WanDisco.</p>
<p>Like NNBench, <a href="https://issues.apache.org/jira/browse/HDFS-708">S-Live is more designed</a> to test the file operations of HDFS, and not necessarily focused on the bulk transfer ability of an HDFS cluster. S-Live creates, reads, renames, and deletes files, but goes beyond the operations from NNBench and also appends to files, creates directories, and lists the contents of directories. </p>
<p>S-Live departs from NNBench in two major ways. First, the data written in the files is the output of a hash function over an easily computed sequence, which allows the benchmark to verify the data it reads from an earlier write step. Second, rather than each map task operating a benchmark in an orderly fashion and in isolation from the other map tasks by working in a subdirectory, in S-Live map tasks can and do interact with the same files in parallel. S-Live copes with the parallelism by embracing failure; an operation like a read might well fail because the file has already been deleted by another map task. S-Live does not attempt to coordinate operations between map tasks.    </p>
<p>The S-Live benchmark, like NNBench, algorithmically creates its environment on the filesystem, which reduces the amount of state the benchmark needs to maintain. The benchmark is built around a function that can generate a path to a file or directory from a sequence number. When S-Live wants to operate on a file, it pulls a random number between 0 and the maximum number of files in the benchmark, and can instantly derive the full path that it should use to identify that file. S-Live does not have to walk the directory tree or otherwise interrogate the file system to choose which file to use, nor keep an in-memory mapping table of files available. </p>
<p>Whereas NNBench ordered its operations as first all of the writes, then all of the reads, and so on, S-Live randomly selects which operation it will perform next. Each operation can be selected with equal likelihood, or the user may give each class of operation a different weighting to select the ratio each operation will receive. Because operations are selected at random, and the files to operate on are also selected at random, operations in S-Live frequently "fail" when they attempt to operate on files that do not yet exist, or that existed earlier but have been modified or deleted by another map task running the benchmark. To mitigate this, the distribution of operations over time can be biased. For example, the user may wish to have file creates and writes occur more frequently in the early parts of the benchmark, to make later reads and deletes more likely to succeed. In the extreme, an S-Live user may run the benchmark with a 100% write operation mix as a way to create a populated filesystem before starting a second run of the benchmark with additional operations enabled.      </p>
<p>S-Live is included in the mainline Hadoop code, found in <code>hadoop-mapreduce-client-jobclient/src/test/j.o.a.h/fs.slive</code>. WanDisco has a <a href="http://blogs.wandisco.com/2013/02/12/running-slive-test-on-wdd/">blog post with an example of how to run it</a>, which should apply to just about any Hadoop distribution. Shvachko also has a more <a href="https://issues.apache.org/jira/secure/attachment/12448004/SLiveTest.pdf">detailed design document as part of the JIRA</a>. 
Hortonworks also uses it to <a href="http://hortonworks.com/blog/delivering-on-hadoop-next-benchmarking-performance/">test the performance of the NameNode</a></p>
<h4><a name="loadgen"></a>LoadGenerator</h4>
<p>The LoadGenerator tool can be used as a NameNode benchmark. It runs as a stand-alone tool, not as a MapReduce job, and stresses the NameNode through the DFS client libraries. LoadGenerator creates a number of threads. Each thread runs in a loop, randomly picking between read, write, and list file operations, with an optional probability mix for the operations. The benchmark keeps an in-memory list of the files and directories it can potentially operate on, discovered at benchmark startup time, and randomly selects one to act on at each step. For file creates, it randomly selects a directory in which to create the file, and then randomly chooses a file size, where the file size comes from a gaussian distribution with an average file size of 2 blocks and a 1 block standard deviation.   </p>
<p>LoadGenerator runs for a given wall clock time, and reports a synopsis of the performance of each of the different operation types at the end. The benchmark can also be given a script that specifies how the probabilities of the different operations should change over different intervals, so you can run a simulation that has different access patterns over time. </p>
<p>The code is found in <code>hadoop-common/src/test/java/org/apache/hadoop/fs/loadGenerator/</code>. It is likely not a more compelling benchmark than NNBench or S-Live, though the scriptability may be beneficial for some experiments. </p>
<h4><a name="nnthroughput"></a>NNThroughputBenchmark</h4>
<p>One of the <a href="https://issues.apache.org/jira/browse/HADOOP-2149">earliest NameNode Benchmarks</a> is the NNThroughputBenchmark. Also by Shvachko, NNThroughputBenchmark does not stress the NameNode through a MapReduce job, or even through the HDFS Client libraries. Instead, NNThroughputBenchmark invokes operations on the NameNode directly, bypassing the networking and RPC interface. The core NameNode object runs in the same process as the benchmark; there is no client or server. Like NNBench, the NNThroughputBenchmark runs through a set of distinct phases where it tests the same operation over many different files. NNThroughputBenchmark benchmarks creating file, opening a file, deleting a file, checking a file's status, and renaming a file. </p>
<p>NNThroughputBenchmark also exercises the block management functions of the NameNode. The benchmark simulates a number of DataNodes, and again directly invokes the functions inside the NameNode, rather than operating the full protocol between the NameNode and the DataNodes. </p>
<p>The main limitation of the NNThroughputBenchmark is everything operates inside the same Java process. Therefore, benchmark is limited to a single node, and that node must support both the benchmark driver operations and the NameNode object. The benchmark is threaded, so it can take advantage of multiple CPUs in a node to run multiple benchmark driver threads and offer more load to the NameNode object. As the benchmark code is relatively simple, combined with the bypassing of the RPC layer, it seems as least possible that the NameNode object is the bottleneck in the benchmark, and adding additional nodes to the test would not result in additional throughput. (There was some work in a <a href="https://issues.apache.org/jira/browse/HDFS-2945">JIRA by Todd Lipcon</a> to start down the path towards a multi-node version of the benchmark, however, that work has not been pursued.) </p>
<p>NNThroughputBenchmark is part of the standard Hadoop distributions, and is found in <code>hadoop-hdfs/src/test/j.o.a.h/hdfs.server.namenode</code>. There is recent work to make it <a href="https://issues.apache.org/jira/browse/HDFS-5068">use the standard Hadoop tool interface</a> and be a little more user-friendly. Shvachko describes it briefly in his <a href="https://www.usenix.org/legacy/publications/login/2010-04/openpdfs/shvachko.pdf">paper in USENIX ;login</a></p>
<h4><a name="testeditlog"></a>TestEditLog</h4>
<p>To a first approximation, the NameNode is really an in-memory database with mutations persistently stored in a write-ahead log. For many operations, performance will be dominated by how fast updates can be logged to disk. </p>
<p>Studying how the underlying datastore affects filesystem performance is somewhat common, for a recent example see <a href="http://storageconference.org/2010/Papers/SNAPI/6.Stender.pdf">BabuDB</a>, which is part of <a href="http://www.xtreemfs.org/">XtreemFS</a>. For a Hadoop microbenchmark, Ivan Mitic offered a <a href="http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-user/201306.mbox/%3C5e27c946e89c4146acd7009b2e09cc74%40BLUPR03MB602.namprd03.prod.outlook.com%3E">suggestion to use the TestEditLog test</a> as a way to study just the performance of the write-ahead log.</p>
<p>TestEditLog is certainly not a nice, prepackaged benchmark. It's found in the <code>hadoop-hdfs/src/test/j.o.a.h/hdfs.server.namenode</code> package, and for the most part consists of a number of tests to recover from errors in the log. A NameNode benchmarking effort will only find it useful as a starting point.   </p>
<h4><a name="qfs"></a>QFS Benchmark</h4>
<p>HDFS is the best-known of the filesystems that draw inspiration from the Google File System paper, but it is not the only one. Another well-known system was the <a href="http://en.wikipedia.org/wiki/CloudStore">Kosmos FileSystem</a>, from Kosmix (now known as <a href="http://www.walmartlabs.com/">@WalmartLabs</a>). The Kosmos FileSystem served as the basis for the <a href="http://quantcast.github.io/qfs/">Quantcast File System</a>. </p>
<p>The Quantcast Team has some <a href="https://github.com/quantcast/qfs/wiki/Performance-Comparison-to-HDFS">performance benchmarks comparing QFS to HDFS</a>. We'll skip over the data throughput experiments, and instead focus on the metadata experiments. Quantcast makes the <a href="https://github.com/quantcast/qfs/tree/master/benchmarks/mstress">code for their 'mstress' application available on github</a></p>
<p>The mstress benchmark is meant to be run from multiple hosts simultaneously. Rather than a MapReduce job, mstress includes a number of Python scripts to help prepare and launch the benchmark from the different hosts with help from ssh. The same Python code launches benchmarks for either filesystem, but the benchmark for HDFS is written in Java, and the benchmark for QFS is written in C++. </p>
<p>Mstress does not assume that there are Hadoop DataNodes available (or ChunkServers, the QFS equivalent) and so it only tests directory operations like creating, listing, stating, and deleting, which do not require any blocks to be allocated. (At one point, <a href="https://issues.apache.org/jira/browse/HADOOP-2078">HDFS did not permit empty files</a>, but that hasn't been true for a long time. It may be that QFS does not permit them.)  </p>
<p>The benchmark is given a few parameters that describe the directory tree on which it should operate, such as how many levels of directories there should be, and how many subdirectories per level. The benchmark does not need to maintain much state: the creation benchmark builds the entire tree, the list directory benchmark walks the tree and only keeps a count of directories encountered. The stat benchmark, which will randomly select some subset of the directories to examine, can generate paths that it wishes examine on demand and does not need a list a priori. Benchmarks running on different hosts work in different subdirectories so nodes do not interfere with each other's operations. </p>
<p>The mstress benchmark is a promising start, but would greatly benefit from operations beyond directory manipulation. </p>
<h4><a name="ohiostate"></a>Ohio State Infiniband benchmark</h4>
<p>Recently, a group has come together calling itself the <a href="http://clds.ucsd.edu/bdbc">Big Data Benchmarking Community</a>. As one would expect, "Big Data Benchmarking" is a fairly expansive term, and the community has been exploring just what is in and out of the scope of their common interests. The BDBC holds frequent conference calls, and has held a workshop about every 6 months since the beginning of 2012. </p>
<p>Hadoop has been well-covered in the the community. At the 2nd workshop, the <a href="http://hadoop-rdma.cse.ohio-state.edu/publications/">Network Based Computing Lab</a> of the Ohio State University presented some of their work on <a href="http://conferences.computer.org/sc/2012/papers/1000a058.pdf">HDFS over InfiniBand</a>, including some <a href="http://www.paralleldatageneration.org/download/wbdb/WBDB2012_IN_15_Panda_HDFSBenchmark.pdf">microbenchmarking results</a>. Their suite of microbenchmarks target five different measurements:
Sequential Write Latency, Sequential or Random Read Latency, Sequential Write Throughput, Sequential Read Throughput, and Sequential Read-Write Throughput. </p>
<p>It's hard to say exactly what each test does, given only the slides, but the two latency tests appear to read or write a single large file to both 4 and 32 DataNodes, and the Throughput experiments appear to be with multiple DataNodes and multiple clients. The tests compare performance over a 32Gbps Infiniband and 1 Gigabit Ethernet. InfiniBand always wins, but not by nearly as much as one would expect a networking interconnect that is so much faster than Ethernet. (Each node appears to only have one disk, which is an odd balance and may be the bottleneck)</p>
<p>The benchmark is not yet available. The Network-Based Computing Lab hopes to release it as part of their next release, sometime in early fall of 2013. </p>
<h4><a name="swim"></a>SWIM</h4>
<p>Most of the benchmarks we have examined thus far have been HDFS focused, but we make an exception for the <a href="https://github.com/SWIMProjectUCB/SWIM/wiki">Statistical Workload Injector for MapReduce</a> (SWIM) toolset. SWIM is designed to help create MapReduce benchmarks that are statistically similar to real workloads. It eschews attempting to reduce a MapReduce workload to any well-known distribution, and instead uses the set of traces from a production workload as the "model". When SWIM generates a benchmark, it ensures that the workload shares the same characteristics as the production workload, scaled for whatever size cluster on which the user wishes to run the benchmark.</p>
<p>SWIM operates in several phases. In the first phase, SWIM includes a tool to load the full set of logfiles of a target MapReduce workload. This data consists of the runtime of the job (both overall, and broken down into "MapSeconds" and "ReduceSeconds", which are the sums of all the Map Task running times and Reduce Task running times), the amount of data processed by the Map, Shuffle, and Reduce phases, as well as information about the job arrival rate. A second tool samples from this data set to produce a smaller workload that is similar to the overall data set over several dimensions. </p>
<p>The SWIM benchmark ships with summarized workloads originally derived from Facebook production workloads. Cloudera also internally uses summaries derived from their customer workloads, however, those have not yet been made available. (Yahoo reports that they are making <a href="http://clds.sdsc.edu/sites/clds.sdsc.edu/files/wbdb2012/papers/WBDB2012Paper44Chakravarthi.pdf">several of their traces available</a>, which could potentially be additional example data for SWIM)  </p>
<p>In the second phase, SWIM can take the summarized workloads and produce a set of MapReduce jobs that "replay" the summary, potentially with a new cluster configuration. First, it creates enough synthetic input data that it can satisfy the data reads of every job. Then, it generates a set of MapReduce jobs, and a script to launch them with an inter-job arrival rate derived from the original workload. These generated MapReduce jobs read from the synthetic data, and pass along appropriately sized shuffle and reduce data. </p>
<p>To close the loop, the same tools used in the first phase which produce a summary of a MapReduce workload can be run on the raw output of the second phase, and in turn produce a summary that can be compared to the original summary.</p>
<p>Being able to build workloads that are representative of a user's actual workload is clearly desirable. Unfortunately, as an HDFS metadata benchmark, SWIM falls short. The log files of a MapReduce job contain limited information about the file system interactions. At best, the log files contain the directories used for inputs, and the total amount of data processed. There is no information about the characteristics of individual files and no way for SWIM to build a similar structure. If a MapReduce job reads a directory full of 1 million files, each one megabyte, SWIM stores that this was a job that read a terabyte. When it generates synthetic data, it may very well create many fewer but larger files, so long as a terabyte of data is available.   </p>
<h3><a name="newbenchmark"></a>Towards a new benchmark</h3>
<p>We believe that the time is right for a new benchmark to help understand the performance of filesystem metadata management in a distributed storage system. Targeting this benchmark against the Hadoop NameNode is one important case, but an ideal benchmark would not be tied explicitly to Hadoop.  In this section, we describe what we see as important in a new benchmark, borrowing aspects and examples from the benchmarks listed above as appropriate. </p>
<h4><a name="qual1"></a> Desired Quality 1: Portable and easily implementable</h4>
<p>HDFS is a core staple of Hadoop. At the initial release, Hadoop was only a library for writing MapReduce style programs, an execution engine to run those programs, and HDFS to organize data into files, directories, and blocks for MapReduce programs. Since that first release, the world around HDFS has grown more complicated, and some of proposed <a href="http://gigaom.com/2012/07/11/because-hadoop-isnt-perfect-8-ways-to-replace-hdfs/">moving beyond HDFS</a> with new file systems, or adapting Hadoop to use existing parallel file systems or storage systems. </p>
<p>To keep the focus somewhat grounded, we will only consider storage systems that "look like filesystems." Data can can be managed in other forms, such as database tables, object graphs like <a href="http://en.wikipedia.org/wiki/Core_Data">Core Data</a>, or iterators over streaming data, but we won't try to propose a benchmark that could be used to measure the metadata management of those systems. </p>
<p>To put this in concrete terms, we envision a benchmark with fundamental operations like creating files and directories, opening files for reading or writing, listing directories, adding data to files, renaming and deleting files.  </p>
<p>Programs using the Hadoop libraries can use the <code>org.apache.hadoop.fs.FileSystem</code> class to interface to many filesystems, beyond just HDFS. The Hadoop Compatible FileSystem(HCFS) standard is an effort to <a href="https://wiki.apache.org/hadoop/HCFS">more formally specify the semantics</a> that this interface will provide. HCFS is tracking <a href="http://www.datastax.com/dev/blog/cassandra-file-system-design">CassandraFS</a>, <a href="http://ceph.com/docs/next/cephfs/hadoop/">CephFS</a>, <a href="http://www.cleversafe.com/news-reviews/cleversafe-press-releases/2012-press-releases/cleversafe-first-to-deliver-breakthrough-capabilities-for-combined-storage-and-massive-computation">CleverSafe Object Storage</a>, <a href="https://github.com/gluster/hadoop-glusterfs">GlusterFS</a>, <a href="http://answers.mapr.com/questions/116/is-mapr-wire-compatible-or-api-compatible-with-hadoop-0202">MapR Filesystem</a>, <a href="https://github.com/quantcast/qfs/wiki/Migration-Guide">QFS</a>, the <a href="http://www.symantec.com/enterprise-solution-for-hadoop">Symantec Veritas Cluster Fileystem</a>, and should likely add <a href="https://github.com/amplab/tachyon/wiki">Tachyon</a> from UC-Berkeley, and <a href="http://blogs.msdn.com/b/silverlining/archive/2013/01/29/azure-vault-storage-in-hdinsight-a-robust-and-low-cost-storage-solution.aspx">Azure Storage Vaults</a>.</p>
<p>By trying to stay as close as possible to a level that can be easily expressed using operations included in the HCFS contract, a benchmark can move between systems, using the <code>Filesystem</code> in the worst case, but also potentially using an interface native to that system. </p>
<p>A benchmark may wish to target some operations that cannot be expressed through the Filesystem class. For example, the NameNode must process block information from the various DataNodes. We would like to be able to include this a benchmark, but that interface is not exposed through the Filesystem class. </p>
<h4><a name="qual2"></a>Desired Quality #2: Don't require the full stack</h4>
<p>The NameNode is unique in Hadoop in that it is largely a self-contained black box. After a given sequence of calls, it will be in a known state. The identity of the originator of those calls is somewhat irrelevant: If a call says 'node 23 wants to store a block', the NameNode does not go to great lengths to verify that node 23 is in fact the one making the request. This idea is embraced by the NNThroughputBenchmark, which creates an entirely synthetic environment in which the NameNode can operate, entirely oblivious to the fact that there is no cluster. </p>
<p>This flexibility makes it easier to construct and operate benchmarks that demonstrate the core performance challenges, without requiring a large support infrastructure. Consider the TestEditLog example: if the question is how fast can the NameNode log data to disk, which will bound the number of operations per second the NameNode can sustain, we do not need a full HDFS cluster available to determine that number. Similarly, with systems like <a href="https://github.com/lalithsuresh/Scaling-HDFS-NameNode">KTHFS</a>, which uses a Relational Database Management System to store the state of a NameNode, being able to construct a benchmark that simply issues the appropriate query to the underlying database will also allow a benchmarker to understand the lower bound of what the worst case performance might be for the system, without the challenge of building the full system. </p>
<h4><a name="qual3"></a>Desired Quality #3: Separate data generation from benchmark driver</h4>
<p>One of the common themes in existing NameNode benchmarks is that the file and directory structure they interact with is relatively simple, and almost always computed on demand with little state used to track it. By reducing the amount of work the benchmark needs to do to compute the environment, there is less likelihood that there is any overhead polluting the benchmark's measurements. </p>
<p>However, there is no reason that the benchmark needs to combine the operation execution driver from the code that determines the operations to perform. In our ideal benchmark, the benchmark driver can optionally be given a pre-computed scenario of operations to perform, and be set forth on executing them as fast as the underlying system will accept the operations in an "operation execution" phase. By decoupling the two phases, the "scenario generation" phase of determining which operations and on which files to run can be computed ahead of time. Computing these operations off-line allows the benchmark to construct more complicated scenarios and use whatever resources and time it needs to compute the scenario. </p>
<h4><a name="qual4"></a>Desired Quality #4: Operate on a mix of input data</h4>
<p>By decoupling the benchmark into a "scenario generation" phase and a "operation execution" phase, the scenarios to be benchmarked can come from a variety of sources. </p>
<p>There is little published on exactly what operations a NameNode encounters in typical deployment -likely because there is not really anything like a "typical" HDFS use case. One of the few examples available is <a href="http://hadoopblog.blogspot.com/2010/04/curse-of-singletons-vertical.html">from Facebook in 2010</a>, which classified the distribution of operations at the NameNode as:</p>
<ul>
<li>stat a file or directory 47%</li>
<li>open a file for read 42%</li>
<li>create a new file 3%</li>
<li>create a new directory 3%</li>
<li>rename a file 2%</li>
<li>delete a file 1% </li>
</ul>
<p>That does not mean that a benchmark should fix its operation frequency to match only that distribution. Instead, we want to support multiple scenarios. We may want to be able to replay a trace from an exact workload, or we may want to replay a trace synthesized through something like SWIM. We may also want to try a scenario that we know is unrealistic but that still shows some pathological behavior. </p>
<h4><a name="qual5"></a>Desired Quality #5: Expose parallelism, but capture dependencies</h4>
<p>The NameNode or any other metadata management system must be able to handle concurrent operations, and indeed, many of the performance issues of the NameNode come from handling the locking operations required to maintain its internal consistency. Therefore, any benchmark must issue some number of simultaneous operations. </p>
<p>The challenge is how to issue the operations in a realistic fashion. We expect that the NameNode will respect the C in ACID, but it is up to the application (and hence the benchmark) to ensure that the states the NameNode is moving between are consistent to be begin with. For example, an application would not in a normal setting try to create a file before creating the parent directory for that file. Existing benchmarks cope with this in different ways. Some, like NNBench, carefully order their operations, to ensure that each operation has the correct state to succeed. Others, like S-Live, are intentionally oblivious and in fact consider testing the error conditions to be an important part of the benchmark. </p>
<p>We believe a benchmark should take the middle ground. Scenarios to be executed will have dependency information included - a file create operation will depend on the creation of the parent directory. The benchmark will use the dependency information to create a serializable schedule of operations. It will be up to the benchmarker to ensure that the trace provides enough potential parallelism in the scenario to adequately exercise the benchmark.    </p>
<h4><a name="qual6"></a>Desired Quality #6: Deterministic</h4>
<p>Benchmarks should be repeatable. For many of the existing benchmarks, this is accomplished by running the benchmark with the same parameters. Others may need to ensure the same seed is fed to the random number generator to produce the same sequence of events. In a trace-driven benchmark, much of the work of determinism is easily handled: there is only one set of decisions to be made about which operation to perform next. </p>
<p>However, to make it easier for the benchmark to stay deterministic while running at high speed, there are some small changes to Hadoop that could be helpful. Not every call to the NameNode returns the same result: allocating a new block gets a new block identifier from the NameNode, and the NameNode makes no guarantees about which nodes it may chose to place block replicas. </p>
<p>Allowing the client to provide hints or outright requirements for what to call blocks or on which nodes to place blocks would make a trace replay much easier. For block identifiers, the NameNode would still be responsible for ensuring the ID is valid and not already in use. Then, rather than having to track what the NameNode decides to assign for a block identifier, the benchmark driver can just ensure that it got back what it expected. This extension is helpful for situations like HDFS Federation or Content-Addressable storage systems, where the ID of the block would not be under the control of the NameNode anyway. </p>
<p>As for host placement, the NameNode already allows for a pluggable API in the <code>BlockPlacementPolicy</code> interface to decide where to place a block. Extending the API to allow the client to include hints is a reasonable extension, useful beyond a benchmark. </p>
<h3>Wrapping up</h3>
<p>We'll be updating this post as we find more benchmarks - please let <a href="mailto:epaulson@unit1127.com">me</a> know what I've missed! </p></div>
    <footer>
      <p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Erik Paulson</span>
  </span>
  <time datetime="2013-08-28T10:00:00" pubdate>Wed 28 August 2013</time>  </p>      <div class="sharing">
    <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://epaulson.github.io/HadoopInternals/benchmarks.html" data-via="erik_paulson" data-counturl="http://epaulson.github.io/HadoopInternals/benchmarks.html" >Tweet</a>
      </div>    </footer>
  </article>

  </div>
      <aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
            <li class="post">
          <a href="http://epaulson.github.io/HadoopInternals/benchmarks.html">HDFS Benchmarks</a>
      </li>
            <li class="post">
          <a href="http://epaulson.github.io/HadoopInternals/first-post.html">What this blog will be about</a>
      </li>
          </ul>
  </section>
    <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
              <li><a href="http://epaulson.github.io/category/namenode.html">Namenode</a></li>
          </ul>
  </section>
   

  <section>
  <h1>Tags</h1>
    </section>

    
      <section>
        <h1>Blogroll</h2>
        <ul>
                    <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
                    <li><a href="http://hadoop.apache.org/docs/current/api/overview-summary.html" target="_blank">Hadoop API</a></li>
                    <li><a href="https://issues.apache.org/jira/browse/HADOOP" target="_blank">Hadoop Jira</a></li>
                </ul>
    </section>
  <section>
    <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("erik_paulson", 5, false);
    });
  </script>
  <script src="/theme/js/twitter.js" type="text/javascript"> </script>
        <a href="http://twitter.com/erik_paulson" class="twitter-follow-button" data-show-count="false">Follow @erik_paulson</a>
  </section>
  </aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Erik Paulson -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
    <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>
</body>
</html>